{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, T5TokenizerFast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  My initial idea was fine-tune some T5 model, but in one noght my computer did not managed to make even one epoch\n",
    "```ModelFineTune.py``` is that attempt\n",
    "\n",
    "So now my plan is:\n",
    "\n",
    "### Plan:\n",
    "\n",
    "1. Find T5 model \n",
    "2. Generate the summary for test texts with just pretrained model\n",
    "\n",
    "\n",
    "As for text preprocessing:\n",
    "\n",
    "1. remove any punctuation, numbers and non-russian text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_TEST = './Data/test.csv'\n",
    "\n",
    "MODEL_NAME = 'UrukHan/t5-russian-summarization'\n",
    "MAX_INPUT = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Proprocess texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import string\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punct(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.digits))\n",
    "    return text\n",
    "\n",
    "# keep only russian words\n",
    "def remove_non_russian_words(text):\n",
    "    russian_pattern = re.compile(r'[А-Яа-я]+')\n",
    "    russian_words = russian_pattern.findall(text)\n",
    "    cleaned_text = ' '.join(russian_words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                                          site_text\n",
      "0        0  HANNAH MARTIN\\nОткрой для себя \\nновый стиль с...\n",
      "1        1  Преимущества\\nКак работает?\\nУстановка\\nЗаказа...\n",
      "2        2  +7 (812) 916 49 58\\nПН-ПТ с 10 до 20 \\nНаш Tel...\n",
      "3        3  Previous\\nСовершай платежи \\n в любое время, в...\n",
      "4        4  Остекление балконов\\nАлюминиевое остекление\\nП...\n",
      "...    ...                                                ...\n",
      "1995  1995  Рассчитать стоимость фундамента\\nСтроения\\nКал...\n",
      "1996  1996   Канализация для загородного дома «под ключ»\\n...\n",
      "1997  1997  +7 495 974-79-79\\npartners@itpss.ru\\n5 провере...\n",
      "1998  1998  +7 (499) 499-46-96\\nНабор веса за 4 недели Био...\n",
      "1999  1999  Современный сервис \\n для управления инвестици...\n",
      "\n",
      "[2000 rows x 2 columns]\n",
      "0       Открой для себя новый стиль с Начать покупки Ч...\n",
      "1       Преимущества Как работает Установка Заказать У...\n",
      "2       ПН ПТ с до Наш чатик ТУТ Добро пожаловать Вход...\n",
      "3       Совершай платежи в любое время в любой точке К...\n",
      "4       Остекление балконов Алюминиевое остекление Пла...\n",
      "                              ...                        \n",
      "1995    Рассчитать стоимость фундамента Строения Кальк...\n",
      "1996    Канализация для загородного дома под ключ от о...\n",
      "1997    проверенных лайфхаков для успешного бизнеса на...\n",
      "1998    Набор веса за недели Биорост Форте Ешь так же ...\n",
      "1999    Современный сервис для управления инвестициями...\n",
      "Name: site_text_cleaned, Length: 1999, dtype: object\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_PATH_TEST)\n",
    "print(test_df)\n",
    "test_df = test_df.loc[test_df.site_text.notna()]\n",
    "# Apply the remove_punct function to 'site_text'\n",
    "test_df['site_text_cleaned'] = test_df['site_text'].apply(remove_punct)\n",
    "# Apply the remove_non_russian_words function to 'site_text'\n",
    "test_df['site_text_cleaned'] = test_df['site_text'].apply(remove_non_russian_words)\n",
    "print(test_df['site_text_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Generate banners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "input_sequences = test_df['site_text_cleaned'].values.tolist()\n",
    "task_prefix = \"Spell correct:\"\n",
    "decoded_outputs = []\n",
    "i = 0\n",
    "\n",
    "for sequence in input_sequences:\n",
    "    i += 1\n",
    "    if i % 10 == 0:\n",
    "        print(i)\n",
    "\n",
    "    encoded = tokenizer(sequence, padding=\"longest\", max_length=MAX_INPUT, truncation=True, return_tensors=\"pt\")\n",
    "    encoded.to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        input_ids=encoded['input_ids'],\n",
    "        attention_mask=encoded['attention_mask'],\n",
    "        max_length=15,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    decoded_output = tokenizer.batch_decode(generated_ids.to(\"cpu\"), skip_special_tokens=True)\n",
    "    decoded_outputs.append(decoded_output)\n",
    "\n",
    "decoded_outputs = [output.to(\"cpu\") for output in decoded_outputs]\n",
    "test_df['decoded_outputs'] = decoded_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So now my df has generated banners, in a column \"decoded outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Getting embeddings and making submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "test_df = pd.read_csv('test_with_generated.csv').drop(columns=['Unnamed: 0', 'site_text', 'site_text_cleaned'])\n",
    "\n",
    "# here we had empty site text, so I just made it uo\n",
    "new_row_data = {\n",
    "    'id': '988',\n",
    "    'decoded_outputs': 'Всем привет!',\n",
    "}\n",
    "\n",
    "new_row_df = pd.DataFrame([new_row_data])\n",
    "\n",
    "test_df = pd.concat([test_df.iloc[:988], new_row_df, test_df.iloc[988:]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "embeddings = model.encode(test_df['decoded_outputs'].tolist())\n",
    "flat_embeddings = embeddings.flatten()\n",
    "\n",
    "embedding_df = pd.DataFrame({'id': test_df['id'].repeat(384), 'val': flat_embeddings})\n",
    "embedding_df['siteId_eId'] = [f\"{siteId}_{eId}\" for siteId in range(len(test_df)) for eId in range(384)]\n",
    "embedding_df = embedding_df[['id', 'siteId_eId', 'val']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "embedding_df.to_csv('banner_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_various_tasks-_CCT181H",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
